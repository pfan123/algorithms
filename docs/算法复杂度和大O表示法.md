# 算法复杂度和大O表示法

## 概念

算法复杂度是算法分析里的概念（对应到[SICP](https://book.douban.com/subject/1148282/)里的增长阶），是衡量计算资源消耗数量（例如计算时间，存储器使用等）的指标。

算法的复杂度在理论上表示为一个函数：其定义域是输入数据的长度（通常考虑任意大的输入，没有上界），值域通常是执行步骤数量（时间复杂度）或者存储器位置数量（空间复杂度）。

这个函数形如：

```
R(n) = Θ(f(n))` 亦可记做 `O(f(n))
```

`f(n)` 就是被度量的算法主体；算法的复杂度标记就是大O（Θ读做theta），这种记法称为[大O表示法](https://en.wikipedia.org/wiki/Big_O_notation)。

## 常见复杂度级别

- ** O(1)— ** 常数时间：该算法仅仅使用一步就可以完成任务

- ** O(log n)— ** 对数时间：该算法每执行一步，它完成任务所需要的步骤数目会以一定的因子减少。

- ** O(n)— ** 线性时间：该算法完成任务所需要的步骤直接和n相关（1对1的关系）。

- ** O(n²)—  ** 二次方时间：完成任务所需要的步骤是n的平方。

- ** O(C^n)—  ** 指数时间：完成任务所需要的步骤是一个常数的n次方（非常大的数字）。



[Big-O Cheat Sheet](http://bigocheatsheet.com/)
